# Environment Parameters

domain_name: mjrl_reacher_7dof-v0
task_name: ferm 
reward_type: dense
special_reset: 
cameras: [0, 1]
action_repeat: 1

# RRL/CURL/RAD Training Parameters

agent: rad_sac
encoder_type: pixel
observation_type: hybrid 
pre_transform_image_size: 100 
image_size: 84
frame_stack: 1 
detach_encoder: False
data_augs: crop
latent_dim: 512
encoder_feature_dim: 50
encoder_lr: 1e-3
encoder_tau: 0.05
num_layers: 4
num_filters: 32

num_updates: 1
init_steps: 0
num_train_steps: 6000000
batch_size: 128
hidden_dim: 1024
num_eval_episodes: 25

change_model: False
synch_update: False
two_conv: False
model_dir: 
model_step: 

# Warming up parameters

bc_only: False
warmup_cpc: 1600
warmup_cpc_ema: True
warmup_offline_sac:

# Replay Buffer

replay_buffer_capacity: 1000000
replay_buffer_load_dir: 

# Expert Demos

demo_samples: 2500
demo_model_step: 0
demo_model_dir: ../resnet/hand_dapg/dapg/policies/
demo_special_reset: 
success_demo_only: False

# Sac params

discount: 0.99
init_temperature: 0.1
alpha_lr: 1e-4
alpha_beta: 0.5

# Actor Parameters

actor_lr: 1e-3
actor_beta: 0.9
actor_log_std_min: -10
actor_log_std_max: 2
actor_update_freq: 2

# Critic Parameters

critic_lr: 1e-3
critic_beta: 0.9
critic_tau: 0.05
critic_target_update_freq: 1

# Saving Flags & Logging

save_tb: False
save_buffer: False
save_video: False
save_model: False 
save_sac: False

eval_freq: 20000
log_interval: 100
log_networks_freq: 1000000

seed: 1
work_dir: '.'
hydra:
    run:
        dir: ${work_dir}

